---
layout: post
title:  "Zarr Python 2.3 release"
date:   2019-05-02
categories: zarr python release
---

Recently we released version 2.3 of the Python Zarr package, which
implements the Zarr protocol for storing N-dimensional typed arrays,
and is designed for use in distributed and parallel computing. This
post provides an overview of new features in this release, and some
information about future directions for Zarr.

## New storage options for distributed computing

A key feature of the Zarr protocol is that the underlying storage
system is decoupled from other components via a simple key/value
interface. In Python, this interface corresponds to the
[`MutableMapping` interface](@@TODO), which is the interface that
Python dictionaries (`dict`) implement. The simplicity of this
interface means it is relatively straightforward to add support for a
range of different storage systems. The 2.3 release adds support for
storage using [SQLite](@@TODO), [Redis](@@TODO), [MongoDB](@@TODO) and
[Azure Blob Storage](@@TODO).

For example, here's code that stores an array using Redis:

{% highlight python %}
TODO
{% endhighlight %}

Here's the same example but storing the data in the cloud via Azure
Blob Storage:

{% highlight python %}
TODO
{% endhighlight %}

Support for other cloud object storage storage services was already
available via other packages, with Amazon S3 supported via the
[s3fs](@@TODO) package, and Google Cloud Storage supported via the
[gcsfs](@@TODO) package.

The attraction of cloud storage is that total I/O bandwidth scales
linearly with the size of a computing cluster, so there are no
technical limits to the size of the data or computation you can scale
up to. Here's a slide from a recent presentation from Ryan Abernathy
showing how I/O scales when using Zarr, and comparing that to reading
data from a remote data service which does not scale in the same way:

@@TODO plot

## Optimisations for cloud storage: consolidated metadata

One issue with using cloud object storage is that, although total I/O
throughput can be high, the latency involved in each request to read
the contents of an object can be around 100 ms, even when reading from
compute nodes within the same data centre. This latency can add up
when reading many arrays, because in Zarr each array has its own
metadata stored in a separate object.

To work around this, the 2.3 release adds an experimental feature to
consolidate metadata for all arrays and groups within a hierarchy into
a single object, which can be read once via a single request. Although
this is not suitable for rapidly changing datasets, it can be good for
large datasets which are relatively static.

To use this feature, two new convenience functions have been
added. The [`consolidate_metadata()`](@@TODO) function performs the
initial consolidation, reading all metadata and combining them into a
single object. Once you have done that and deployed the data to a
cloud object store, the [`open_consolidated()`] function can be used
to read data, making use of the consolidated metadata.

Support for this new feature is also now available via the
[xarray](@@TODO) and [intake](@@TODO), and the Pangeo project is using
consolidated metadata for all new Zarr datasets. E.g., here's an
example of how to open a Zarr dataset from Pangeo's data catalog via
intake:

{% highlight python %}
TODO
{% endhighlight %}

## Compatibility with N5

Around the same time that development on Zarr was getting started, a
separate team led by Stephan Saafeld at the Janelia research campus
was experiencing similar frustrations trying to use HDF5 for storing
brain imaging data, and developed a software library called
N5. Although N5 was originally implemented in Java, it has strong
similarities to Zarr in the approach it takes to storing both metadata
and data chunks. We became aware of each others' work around a year
ago and have been learning about the two approaches.

The bottom line is that there is a lot of commonality and we are
working jointly to bring the two approaches together. As a first
experimental step towards that goal, the Zarr 2.3 release includes a
storage adapter which allows reading and writing of data on disk in
the N5 format. Here's an example:

{% highlight python %}
TODO
{% endhighlight %}

## Future developments

There is a growing community of interest around new approaches to
storage of array-like data, and we'd like to do what we can to build
connections and share knowledge and ideas between these
communities. We've started a regular teleconference which is open to
anyone to join, and there is a new gitter channel for general
discussion.

The main focus of our conversations so far has been setting up work
towards development of a new set of specifications that support the
features of both Zarr and N5, and the requirements of key user groups,
including Earth and climate sciences, bioimaging and microscopy, and
data derived from large scale DNA and RNA sequencing experiments. It
is still relatively early days and there are lots of open questions to
work through, both on the technical side and in terms of how we
organise and coordinate efforts. However, the community is very
friendly and supportive, and anyone is welcome to participate, so if
you have an interest please do consider getting involved.

If you would like to stay in touch with where things are going, keep
an eye on the [zarr](@@TODO) and [zarr-specs](@@TODO) GitHub
repositories, join the [gitter channel](@@TODO), and please feel free
to raise issues or add comments if you have any questions or ideas.

## Further reading

* [Zarr Python 2.3 release notes](@@TODO)
* anything else...
